{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da826088",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265b463",
   "metadata": {},
   "source": [
    "Here are some exercises related to probability, Bayes' rule, and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c73554",
   "metadata": {},
   "source": [
    "86% of BU students in the class of 2027 were in the top 10% of their class.  21% of BU students are first-generation.  Assuming these are independent events, calculate the expected number of students who are both in the top 10% of their class and first-generation students, out of a class of 3145."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b962b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567.987\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "P_toc = 0.86\n",
    "P_fg = 0.21\n",
    "P_toc_fg = P_toc * P_fg\n",
    "num_students = 3145\n",
    "print(num_students * P_toc_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d6fd2",
   "metadata": {},
   "source": [
    "Suppose that on looking into the situation further, we found that actually 620 first-generation students were in the top 10% of their class.  Calculate P(TopOfClass | FirstGeneration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c24386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7395999999999999\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "num_toc_fg = 620\n",
    "P_toc_fg_observed = num_toc_fg / num_students\n",
    "P_toc_given_fg = P_toc_fg_observed * P_toc / P_fg\n",
    "print(P_toc_given_fg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7190f",
   "metadata": {},
   "source": [
    "A Naive Bayes spam classifier is trying to decide whether the simple message \"BUY GOLD FROM US\" is spam.  In the training corpus documents that aren't spam, there are 100000 words; BUY shows up 1000 times, GOLD shows up 500 times, FROM shows up 2000 times, and US appears 1500 times.  In the spam training documents, which contain 50000 words, BUY shows up 3000 times, GOLD shows up 1000 times, FROM shows up 2000 times, and US appears 2000 times.  We assume the P(spam) prior is 1/3.  Calculate the probability a Naive Bayes classifier would ascribe to the \"BUY GOLD FROM US\" message being spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b5fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05660377358490565\n",
      "6.666666666666668e-07\n",
      "4e-08\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "P_spam_prior = 1/3\n",
    "\n",
    "P_buy_given_not_spam = 1000/100000\n",
    "P_gold_given_not_spam = 500/100000\n",
    "P_from_given_not_spam = 2000/100000\n",
    "P_us_given_not_spam = 1500/100000\n",
    "\n",
    "P_buy_given_spam = 3000/100000\n",
    "P_gold_given_spam = 1000/100000\n",
    "P_from_given_spam = 2000/100000\n",
    "P_us_given_spam = 2000/100000\n",
    "\n",
    "P_buy_given_spam = 3000/100000\n",
    "P_gold_given_spam = 1000/100000\n",
    "P_from_given_spam = 2000/100000\n",
    "P_us_given_spam = 2000/100000\n",
    "\n",
    "P_message_is_spam = P_spam_prior * P_buy_given_spam * P_gold_given_spam * P_from_given_spam * P_us_given_spam\n",
    "P_message_is_not_spam = (1 - P_spam_prior) * P_buy_given_not_spam * P_gold_given_not_spam * P_from_given_not_spam \n",
    "\n",
    "print(P_message_is_spam / (P_message_is_spam + P_message_is_not_spam))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536c3c4",
   "metadata": {},
   "source": [
    "# Local Search\n",
    "\n",
    "Consider the loss function $L(x,y) = (x - 2)^2 + (y - 3)^2$ and suppose we're trying to minimize it.  Discuss with a partner or the class how this would roughly work in the case of:\n",
    "\n",
    "**Hill Climbing**\n",
    "\n",
    "**Beam Search**\n",
    "\n",
    "Then actually write the code that would do it for **gradient descent** below, starting from an initial solution of (0, 0).  Note that the gradient is $\\begin{bmatrix}2x-4 \\\\ 2y - 6\\end{bmatrix}$, so you want to adjust the tentative solution by a negative multiple of that every round.  Use 0.01 as your learning rate.  Instead of using a stopping criterion, you can just run 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a109a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddddc9",
   "metadata": {},
   "source": [
    "(You might be thinking, \"Hey, I know calculus.  I can just set the derivatives to 0 and solve!\"  That would work here, but not in more complex, bumpy spaces.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
